{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec37825",
   "metadata": {
    "id": "fec37825",
    "outputId": "00be46d1-fa5c-4e68-9abb-67add2bd73de"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy.data import Field, BucketIterator, Iterator\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import nlpaug\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.flow.sequential as naf\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import re\n",
    "from tokenize import tokenize, untokenize\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a34f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f01e4a",
   "metadata": {
    "id": "50f01e4a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"ABAP_NEW_V1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09bc3fdc",
   "metadata": {
    "id": "09bc3fdc",
    "outputId": "c2de635a-9682-4245-c5e5-0cf0f71b9e38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c2f255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pseudo Code               object\n",
       "ABAP(HANA) source code    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0827974d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5b38aa",
   "metadata": {
    "id": "2f5b38aa",
    "outputId": "e21f0b8a-30cf-4267-ed3c-8f692038dd0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pseudo Code</th>\n",
       "      <th>ABAP(HANA) source code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Read from internal table lt_mara to get the data</td>\n",
       "      <td>read table lt_mara from data(ls_data).\\nIf sy-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fetch data from ZMARA using inner join ZMAKTX ...</td>\n",
       "      <td>select * from zmara inner join zmaktx on zmakt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Read data from internal table lt_vbap</td>\n",
       "      <td>read table lt_vbap from data(ls_data).\\nIf sy-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Move data of one table to another table</td>\n",
       "      <td>move-corresponding lt_tab1 to lt_tab2.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loop over internal table lt_cust to get the data</td>\n",
       "      <td>loop at lt_cust into data(ls_data).\\nEndloop.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>check lt_data exists or not</td>\n",
       "      <td>if lt_data is not initial.\\nendif.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>concatenate 2 variables separated by ','</td>\n",
       "      <td>data(lv_val) = |{ lv_value1 },{ lv_value 2}|.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Create selection screen with paramaeter.</td>\n",
       "      <td>SELECTION-SCREEN BEGIN OF BLOCK rad1 WITH FRAM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Read dataset from AL11 path</td>\n",
       "      <td>OPEN DATASET dset FOR OUTPUT IN BINARY MODE.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>declare a standard internal table</td>\n",
       "      <td>data: lt_table type standard table of ztable.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Pseudo Code  \\\n",
       "0     Read from internal table lt_mara to get the data   \n",
       "1    Fetch data from ZMARA using inner join ZMAKTX ...   \n",
       "2                Read data from internal table lt_vbap   \n",
       "3              Move data of one table to another table   \n",
       "4     loop over internal table lt_cust to get the data   \n",
       "..                                                 ...   \n",
       "295                        check lt_data exists or not   \n",
       "296           concatenate 2 variables separated by ','   \n",
       "297           Create selection screen with paramaeter.   \n",
       "298                        Read dataset from AL11 path   \n",
       "299                  declare a standard internal table   \n",
       "\n",
       "                                ABAP(HANA) source code  \n",
       "0    read table lt_mara from data(ls_data).\\nIf sy-...  \n",
       "1    select * from zmara inner join zmaktx on zmakt...  \n",
       "2    read table lt_vbap from data(ls_data).\\nIf sy-...  \n",
       "3               move-corresponding lt_tab1 to lt_tab2.  \n",
       "4        loop at lt_cust into data(ls_data).\\nEndloop.  \n",
       "..                                                 ...  \n",
       "295                 if lt_data is not initial.\\nendif.  \n",
       "296      data(lv_val) = |{ lv_value1 },{ lv_value 2}|.  \n",
       "297  SELECTION-SCREEN BEGIN OF BLOCK rad1 WITH FRAM...  \n",
       "298  OPEN DATASET dset FOR OUTPUT IN BINARY MODE.\\n...  \n",
       "299      data: lt_table type standard table of ztable.  \n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b609d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame()\n",
    "\n",
    "for j in df['ABAP(HANA) source code']:\n",
    "    val = str(j).lower()\n",
    "    \n",
    " \n",
    "    if 'select' in val:\n",
    "        df1  = pd.DataFrame({\"ABAP\":[val],\"Category\":['fetch']})\n",
    "        df_new = df_new.append(df1, ignore_index = True)  \n",
    "    elif 'read' in val:\n",
    "        df1  = pd.DataFrame({\"ABAP\":[val],\"Category\":['Read']})\n",
    "        df_new = df_new.append(df1, ignore_index = True)  \n",
    "    elif 'object' in val:\n",
    "        df1  = pd.DataFrame({\"ABAP\":[val],\"Category\":['object']})\n",
    "        df_new = df_new.append(df1, ignore_index = True)  \n",
    "    elif 'delete' in val:\n",
    "        df1  = pd.DataFrame({\"ABAP\":[val],\"Category\":['delete']})\n",
    "        df_new = df_new.append(df1, ignore_index = True)  \n",
    "    elif 'loop' in val:\n",
    "        df1  = pd.DataFrame({\"ABAP\":[val],\"Category\":['loop']})\n",
    "        df_new = df_new.append(df1, ignore_index = True)  \n",
    "    elif 'call function' in val:\n",
    "        df1  = pd.DataFrame({\"ABAP\":[val],\"Category\":['function module']})\n",
    "        df_new = df_new.append(df1, ignore_index = True) \n",
    "    elif 'rollback' in val or 'commit' in val:\n",
    "        df1  = pd.DataFrame({\"ABAP\":[val],\"Category\":['Database LUW']})\n",
    "        df_new = df_new.append(df1, ignore_index = True)         \n",
    "    else:\n",
    "        df1  = pd.DataFrame({\"ABAP\":[val],\"Category\":['other']})\n",
    "        df_new = df_new.append(df1, ignore_index = True)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4642622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " [Text(0, 0, 'function module'),\n",
       "  Text(1, 0, 'Database LUW'),\n",
       "  Text(2, 0, 'object'),\n",
       "  Text(3, 0, 'loop'),\n",
       "  Text(4, 0, 'Read'),\n",
       "  Text(5, 0, 'delete'),\n",
       "  Text(6, 0, 'fetch'),\n",
       "  Text(7, 0, 'other')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfg = df_new.groupby(['Category']).count().sort_values(by=['ABAP'], ascending=True)\n",
    "\n",
    "dfg.plot(kind='bar', ylabel='Count',\n",
    "         xlabel='Category', figsize=(13, 8))\n",
    "\n",
    "plt.xticks(rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cccdbf4f",
   "metadata": {
    "id": "cccdbf4f",
    "outputId": "477b4a46-b327-4952-9f65-feec1e665788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo-code:\n",
      " Read data from internal table lt_vbap\n",
      "ABAP code:\n",
      " read table lt_vbap from data(ls_data).\n",
      "If sy-subrc = 0.\n",
      "endif.\n"
     ]
    }
   ],
   "source": [
    "print('Pseudo-code:\\n',df['Pseudo Code'][2])\n",
    "print('ABAP code:\\n', df['ABAP(HANA) source code'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54af4493",
   "metadata": {
    "id": "54af4493",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2953e7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50969c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea82e3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['internal',\n",
       " 'table',\n",
       " 'function',\n",
       " 'module',\n",
       " 'class',\n",
       " 'object',\n",
       " 'replace',\n",
       " 'single',\n",
       " 'do',\n",
       " 'loop',\n",
       " 'error',\n",
       " 'index',\n",
       " 'message',\n",
       " 'leave',\n",
       " 'commit',\n",
       " 'call']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_words = ['internal', 'table', 'function', 'module', 'class', 'object', \n",
    "            'replace', 'single', 'do', 'loop', 'error', 'index', 'message', 'leave', 'commit', 'call']\n",
    "key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd782be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.update(key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6b4b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_synonym_aug(word):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet',aug_min=1, aug_max=3, stopwords=stop_words)\n",
    "    return aug.augment(word,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f737c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read data from internal table lt_vbap'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_pse = df['Pseudo Code'][2].lower()\n",
    "org_pse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6631a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['translate data point from internal table lt_vbap',\n",
       " 'record information from internal table lt_vbap',\n",
       " 'record data from internal table lt_vbap',\n",
       " 'read information from internal table lt_vbap',\n",
       " 'record data point from internal table lt_vbap']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synonym_aug(org_pse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b807bf6c",
   "metadata": {
    "id": "b807bf6c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pseudo Code</th>\n",
       "      <th>ABAP(HANA) source code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Read from internal table lt_mara to get the data</td>\n",
       "      <td>read table lt_mara from data(ls_data).\\nIf sy-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fetch data from ZMARA using inner join ZMAKTX ...</td>\n",
       "      <td>select * from zmara inner join zmaktx on zmakt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Read data from internal table lt_vbap</td>\n",
       "      <td>read table lt_vbap from data(ls_data).\\nIf sy-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Move data of one table to another table</td>\n",
       "      <td>move-corresponding lt_tab1 to lt_tab2.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loop over internal table lt_cust to get the data</td>\n",
       "      <td>loop at lt_cust into data(ls_data).\\nEndloop.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>adjudge a stock internal table</td>\n",
       "      <td>data: lt_table type standard table of ztable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>adjudge a received internal table</td>\n",
       "      <td>data: lt_table type standard table of ztable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>adjudge a stock internal table</td>\n",
       "      <td>data: lt_table type standard table of ztable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>hold a stock internal table</td>\n",
       "      <td>data: lt_table type standard table of ztable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>adjudge a received internal table</td>\n",
       "      <td>data: lt_table type standard table of ztable.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Pseudo Code  \\\n",
       "0      Read from internal table lt_mara to get the data   \n",
       "1     Fetch data from ZMARA using inner join ZMAKTX ...   \n",
       "2                 Read data from internal table lt_vbap   \n",
       "3               Move data of one table to another table   \n",
       "4      loop over internal table lt_cust to get the data   \n",
       "...                                                 ...   \n",
       "1795                     adjudge a stock internal table   \n",
       "1796                  adjudge a received internal table   \n",
       "1797                     adjudge a stock internal table   \n",
       "1798                        hold a stock internal table   \n",
       "1799                  adjudge a received internal table   \n",
       "\n",
       "                                 ABAP(HANA) source code  \n",
       "0     read table lt_mara from data(ls_data).\\nIf sy-...  \n",
       "1     select * from zmara inner join zmaktx on zmakt...  \n",
       "2     read table lt_vbap from data(ls_data).\\nIf sy-...  \n",
       "3                move-corresponding lt_tab1 to lt_tab2.  \n",
       "4         loop at lt_cust into data(ls_data).\\nEndloop.  \n",
       "...                                                 ...  \n",
       "1795      data: lt_table type standard table of ztable.  \n",
       "1796      data: lt_table type standard table of ztable.  \n",
       "1797      data: lt_table type standard table of ztable.  \n",
       "1798      data: lt_table type standard table of ztable.  \n",
       "1799      data: lt_table type standard table of ztable.  \n",
       "\n",
       "[1800 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = len(df)\n",
    "for i in range(val):\n",
    "    val1 = word_synonym_aug(df['Pseudo Code'][i])\n",
    "    val2 = df['ABAP(HANA) source code'][i]\n",
    "    for j in val1:\n",
    "        val = j\n",
    "        df1 = pd.DataFrame({\"Pseudo Code\":[val],\"ABAP(HANA) source code\":[val2]})\n",
    "        df = df.append(df1, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4753baa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "588f0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['Pseudo Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77845041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1367, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee2358fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loop at lt_cust into data(ls_data).\\nEndloop.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ABAP(HANA) source code'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4087372c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read table lt_vbap from data(ls_data).\\nIf sy-subrc = 0.\\nendif.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ABAP(HANA) source code'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8df5b",
   "metadata": {},
   "source": [
    "#### io.BytesIO tokenization and untokenization procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b509be36",
   "metadata": {
    "id": "b509be36"
   },
   "outputs": [],
   "source": [
    "def tokenize_ABAP_code_io(python_code_str):\n",
    "    python_tokens = list(tokenize(io.BytesIO(python_code_str.encode('utf-8')).readline))\n",
    "    tokenized_output = []\n",
    "    for i in range(0, len(python_tokens)):\n",
    "        tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "801ebd17",
   "metadata": {
    "id": "801ebd17",
    "outputId": "5c473467-601e-4a9d-c42c-ead1ba08ad4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(62, 'utf-8'), (1, 'loop'), (1, 'at'), (1, 'lt_cust'), (1, 'into'), (1, 'data'), (54, '('), (1, 'ls_data'), (54, ')'), (54, '.'), (4, '\\n'), (1, 'Endloop'), (54, '.'), (4, ''), (0, '')]\n",
      "loop at lt_cust into data (ls_data ).\n",
      "Endloop .\n"
     ]
    }
   ],
   "source": [
    "tokenized_sample = tokenize_ABAP_code_io(df['ABAP(HANA) source code'][4])\n",
    "print(tokenized_sample)\n",
    "print(untokenize(tokenized_sample).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9f97b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(62, 'utf-8'), (1, 'read'), (1, 'table'), (1, 'lt_vbap'), (1, 'from'), (1, 'data'), (54, '('), (1, 'ls_data'), (54, ')'), (54, '.'), (4, '\\n'), (1, 'If'), (1, 'sy'), (54, '-'), (1, 'subrc'), (54, '='), (2, '0.'), (4, '\\n'), (1, 'endif'), (54, '.'), (4, ''), (0, '')]\n",
      "read table lt_vbap from data (ls_data ).\n",
      "If sy -subrc =0. \n",
      "endif .\n"
     ]
    }
   ],
   "source": [
    "tokenized_sample = tokenize_ABAP_code_io(df['ABAP(HANA) source code'][2])\n",
    "print(tokenized_sample)\n",
    "print(untokenize(tokenized_sample).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063c79e",
   "metadata": {},
   "source": [
    "#### WhitespaceTokenizer tokenization and TreebankWordDetokenizer for untokenization procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56c0e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ABAP_code(python_code_str):\n",
    "    from nltk.tokenize import WhitespaceTokenizer    \n",
    "    tknzr = WhitespaceTokenizer()\n",
    "    tokenized_output = tknzr.tokenize(python_code_str)\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b39bfbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenized(tokens):\n",
    "    reconstructedSentence = TreebankWordDetokenizer().detokenize(tokens)\n",
    "    reconstructedSentence = reconstructedSentence\n",
    "    return reconstructedSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45db660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loop', 'at', 'lt_cust', 'into', 'data(ls_data).', 'Endloop.']\n",
      "loop at lt_cust into data(ls_data). Endloop.\n"
     ]
    }
   ],
   "source": [
    "tokenized_sample = tokenize_ABAP_code(df['ABAP(HANA) source code'][4])\n",
    "print(tokenized_sample)\n",
    "print(untokenized(tokenized_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d431e267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'table', 'lt_vbap', 'from', 'data(ls_data).', 'If', 'sy-subrc', '=', '0.', 'endif.']\n",
      "read table lt_vbap from data(ls_data). If sy-subrc = 0. endif.\n"
     ]
    }
   ],
   "source": [
    "tokenized_sample = tokenize_ABAP_code(df['ABAP(HANA) source code'][2])\n",
    "print(tokenized_sample)\n",
    "print(untokenized(tokenized_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37c93e80",
   "metadata": {
    "id": "37c93e80"
   },
   "outputs": [],
   "source": [
    "python_problems_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96b1511d",
   "metadata": {
    "id": "96b1511d",
    "outputId": "29821e80-4ab8-4909-8c42-5090cd4d29ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1367, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_problems_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(val):\n",
    "    words_ns = []\n",
    "    for word in val:\n",
    "    # typecaste each val to string\n",
    "        val = str(word)\n",
    " \n",
    "    # split the value\n",
    "        tokens = val.split()\n",
    "    \n",
    "    # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            new = tokens[i].lower()     \n",
    "            if new not in stopwords:\n",
    "                words_ns.append(new)   \n",
    "            \n",
    "    #import seaborn as sns\n",
    "    #%matplotlib inline\n",
    "    #sns.set(rc={'figure.figsize':(5,5)},)\n",
    "    #sns.set_style('darkgrid')\n",
    "    nlp_words = nltk.FreqDist(words_ns)    \n",
    "    nlp_words.plot(20,cumulative=False)      \n",
    "    nlp_words.tabulate(5)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq(python_problems_df['Pseudo Code']) \n",
    "word_freq(python_problems_df['ABAP(HANA) source code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01e225",
   "metadata": {
    "id": "0c01e225",
    "outputId": "056b45e0-3879-492e-ec30-210b53075170"
   },
   "outputs": [],
   "source": [
    "Input = data.Field(tokenize = 'spacy',\n",
    "            init_token='<sos>', \n",
    "            eos_token='<eos>', \n",
    "            lower=True)\n",
    "\n",
    "Output = data.Field(tokenize = tokenize_ABAP_code,\n",
    "                    init_token='<sos>', \n",
    "                    eos_token='<eos>', \n",
    "                    lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89209d75",
   "metadata": {
    "id": "89209d75"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "msk = np.random.rand(len(python_problems_df)) < 0.85 # Splitting data into 85% train and 15% validation\n",
    "\n",
    "train_df = python_problems_df[msk]\n",
    "val_df = python_problems_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58563f0",
   "metadata": {
    "id": "b58563f0",
    "outputId": "d12e2d41-a809-4991-9012-6c9cfe87e494"
   },
   "outputs": [],
   "source": [
    "print(train_df.shape[0])\n",
    "print(val_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a45f07",
   "metadata": {
    "id": "35a45f07"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938f036",
   "metadata": {
    "id": "2938f036",
    "outputId": "9a620c51-f1f4-4f8b-fc1f-108ea1954aaa"
   },
   "outputs": [],
   "source": [
    "fields = [('Input', Input),('Output', Output)]\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81bc28",
   "metadata": {
    "id": "77437e39",
    "outputId": "f741f8ff-1d7f-459d-8cb8-bbc5f359fc8a"
   },
   "outputs": [],
   "source": [
    "train_example = []\n",
    "val_example = []\n",
    "\n",
    "#train_expansion_factor = 100\n",
    "#for j in range(train_expansion_factor):\n",
    "for i in range(train_df.shape[0]):\n",
    "    try:\n",
    "        ex = data.Example.fromlist([train_df['Pseudo Code'][i], train_df['ABAP(HANA) source code'][i]], fields)\n",
    "        train_example.append(ex)\n",
    "    except:\n",
    "         pass\n",
    "\n",
    "for i in range(val_df.shape[0]):\n",
    "    try:\n",
    "        ex = data.Example.fromlist([val_df['Pseudo Code'][i], val_df['ABAP(HANA) source code'][i]], fields)        \n",
    "        val_example.append(ex)\n",
    "    except:\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce5c9d",
   "metadata": {
    "id": "39a0daeb"
   },
   "outputs": [],
   "source": [
    "train_data = data.Dataset(train_example, fields)\n",
    "valid_data = data.Dataset(val_example, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b4bfd",
   "metadata": {
    "id": "08f03602"
   },
   "outputs": [],
   "source": [
    "Input.build_vocab( train_data, min_freq = 0)\n",
    "Output.build_vocab(train_data, min_freq = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4927b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Input.vocab\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a654c16",
   "metadata": {
    "id": "9a654c16"
   },
   "outputs": [],
   "source": [
    "def save_vocab(vocab, path):\n",
    "    output = open(path, 'wb')\n",
    "    pickle.dump(vocab, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98829e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Input.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Output.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a6533",
   "metadata": {},
   "source": [
    "### Data visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcea83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(word_df):\n",
    "    comment_words = ''\n",
    "\n",
    "# iterate through the csv file\n",
    "    for val in word_df:\n",
    "     \n",
    "        # typecaste each val to string\n",
    "        val = str(val)\n",
    " \n",
    "        # split the value\n",
    "        tokens = val.split()\n",
    "     \n",
    "        # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "\n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "    # plot the WordCloud image                      \n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud(python_problems_df['Pseudo Code']) \n",
    "word_cloud(python_problems_df['ABAP(HANA) source code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df = python_problems_df[:200]\n",
    "fig = px.treemap(top_df, title='Treemap',\n",
    "                 path=['ABAP(HANA) source code','Pseudo Code'], \n",
    "                 color='Pseudo Code',\n",
    "                 color_continuous_scale='RdBu')\n",
    "                 \n",
    "fig.update_traces(root_color=\"lightgrey\")\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22803c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = python_problems_df \n",
    "df_1['ABAP(HANA) source code']=df_1['ABAP(HANA) source code'].astype('category').cat.codes\n",
    "df_1['Pseudo Code']=df_1['Pseudo Code'].astype('category').cat.codes\n",
    "corr = df_1.corr()\n",
    "sns.heatmap(corr,linecolor='blue')\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(python_problems_df['ABAP(HANA) source code'], python_problems_df['Pseudo Code'], label= \"ABAP vs Pseudo\", \n",
    "             s=20, cmap = 'viridis', c='blue')\n",
    "  \n",
    "# x-axis label\n",
    "plt.xlabel('ABAP(HANA) source code')\n",
    "# frequency label\n",
    "plt.ylabel('Pseudo Code')\n",
    "# plot title\n",
    "plt.title('Scatter plot')\n",
    "# showing legend\n",
    "plt.legend()\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37acfe",
   "metadata": {
    "id": "ee37acfe",
    "outputId": "50f6831e-da60-4b85-a772-b9ef989b7a05"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49196bdb",
   "metadata": {},
   "source": [
    "#### Encoder-Decoder, Multihead attention, Positional forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab258d87",
   "metadata": {
    "id": "ab258d87"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a534ed",
   "metadata": {
    "id": "33a534ed"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len] \n",
    "                \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701f3a6",
   "metadata": {
    "id": "5701f3a6"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91393c",
   "metadata": {
    "id": "1c91393c"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ab6e8",
   "metadata": {
    "id": "4c7ab6e8"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "                            \n",
    "        #pos = [batch size, trg len]\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "                \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32465370",
   "metadata": {
    "id": "32465370"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        # query, key, value\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9ea45",
   "metadata": {
    "id": "51f9ea45"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d95287",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Input.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0944bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Output.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2239fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(Input.vocab)\n",
    "OUTPUT_DIM = len(Output.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 16\n",
    "DEC_HEADS = 16\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "LEARNING_RATE = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5991c93",
   "metadata": {
    "id": "d5991c93"
   },
   "outputs": [],
   "source": [
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf3871",
   "metadata": {
    "id": "cadf3871"
   },
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = Input.vocab.stoi[Input.pad_token]\n",
    "TRG_PAD_IDX = Output.vocab.stoi[Output.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5cbc0",
   "metadata": {
    "id": "a8d5cbc0",
    "outputId": "d859507f-f0ac-443c-8c36-14e2016cabbc"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213984fb",
   "metadata": {
    "id": "213984fb"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16964cb2",
   "metadata": {
    "id": "16964cb2"
   },
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501a6d1",
   "metadata": {
    "id": "8501a6d1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e8363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(iterator)): #enumerate(iterator):\n",
    "        \n",
    "        src = batch.Input.permute(1, 0)\n",
    "        trg = batch.Output.permute(1, 0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(tqdm(iterator)): #enumerate(iterator):\n",
    "\n",
    "            src = batch.Input.permute(1, 0)\n",
    "            trg = batch.Output.permute(1, 0)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f26c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "epoch_loss = pd.DataFrame()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_example = []\n",
    "    val_example = []\n",
    "\n",
    "    for i in range(train_df.shape[0]):\n",
    "        try:\n",
    "            ex = data.Example.fromlist([train_df['Pseudo Code'][i], train_df['ABAP(HANA) source code'][i]], fields)\n",
    "            train_example.append(ex)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i in range(val_df.shape[0]):\n",
    "        try:\n",
    "            ex = data.Example.fromlist([val_df['Pseudo Code'][i], val_df['ABAP(HANA) source code'][i]], fields)\n",
    "            val_example.append(ex)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "    train_data = data.Dataset(train_example, fields)\n",
    "    valid_data =  data.Dataset(val_example, fields)\n",
    "    \n",
    "    BATCH_SIZE = 16\n",
    "    train_iterator, valid_iterator = BucketIterator.splits((train_data, valid_data), batch_size = BATCH_SIZE, \n",
    "                                                                sort_key = lambda x: len(x.Input),\n",
    "                                                                sort_within_batch=True, device = device)\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    df_epoch   = pd.DataFrame({\"Epoch\":[epoch+1],\"Val_Loss\":[valid_loss], \"Train_Loss\":[train_loss]})\n",
    "    epoch_loss = epoch_loss.append(df_epoch, ignore_index = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "print(f'| Test  Loss: {test_loss:.3f} | Test  PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741fd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss[\"Train_Loss\"], 'g', label='Training loss')\n",
    "plt.plot(epoch_loss[\"Val_Loss\"], 'b', label='Testing loss')\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be81ea0",
   "metadata": {
    "id": "1be81ea0"
   },
   "outputs": [],
   "source": [
    "SRC = Input\n",
    "TRG = Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64156568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxMinWords(paragraph):\n",
    "    numWords = [len(sentence.split()) for sentence in paragraph.split('.')]\n",
    "    return max(numWords), min(numWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32367d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ABAP(HANA) source code'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8533e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['ABAP(HANA) source code']:\n",
    "    print(df['ABAP(HANA) source code'][i])\n",
    "    max, min = MaxMinWords(df['ABAP(HANA) source code'][i])\n",
    "    print(max,min)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2725f0b",
   "metadata": {
    "id": "d2725f0b"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 1000):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "        #print(tokens)\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "        #print(tokens)\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    #print(tokens)\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    #print(src_indexes)\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "        #print(enc_src)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    #print(trg_tokens)\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492832bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_p(text):    \n",
    "    p = \"([i|l|g|y|z]\\w+)\"\n",
    "    result = re.findall(p, text,re.IGNORECASE)\n",
    "    val = []\n",
    "    for i in result:\n",
    "        if i.find('_') != -1: \n",
    "            val.append(i.upper()) \n",
    "            \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5491eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_a(act_var,text):\n",
    "    new_doc = ''\n",
    "    new_doc = text.strip('<eos>').rstrip(\" \")\n",
    "    doc = nlp(text)\n",
    "    expression = r\"(((i|l|g)t)|z)\\w+\"\n",
    "    for i in act_var:        \n",
    "        for match in re.finditer(expression, doc.text,re.IGNORECASE):\n",
    "            start, end = match.span()\n",
    "            span = doc.char_span(start, end)\n",
    "    \n",
    "            if span is not None:  \n",
    "                ##print(span.text)\n",
    "                new_doc = new_doc.replace(span.text,i)\n",
    "            \n",
    "    return new_doc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747aa00d",
   "metadata": {
    "id": "747aa00d",
    "outputId": "c33508a6-266d-4f1e-d460-04ee43160e83",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def conv(text, actual_abap_code):\n",
    "    src = act_text = text\n",
    "    src=src.split(\" \")\n",
    "    translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "    print(f'predicted trg sequence: ')\n",
    "    print(translation)\n",
    "    generated_code = untokenized(translation)\n",
    "#generated_code = untokenize(translation[:-1]).decode('utf-8')\n",
    "#print(\"code: \\n\", generated_code)\n",
    "    generated_code = generated_code.replace('<eos>', \"\")\n",
    "    print(\"code:\\n\", generated_code)    \n",
    "    rev_generated_code = val_a(val_p(act_text),generated_code)\n",
    "    print(\"Revised code:\\n\", rev_generated_code)\n",
    "    data = [[actual_abap_code,rev_generated_code]]\n",
    "    df2 = pd.DataFrame(data, columns=['Actual ABAP code ref', 'Generated ABAP Code'])\n",
    "    new = rev_generated_code.split(\" \")\n",
    "    a = new\n",
    "    a = a.append('<eos>')\n",
    "    return  df2,src,new, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(ref_code,generated_code):\n",
    "    ref = ref_code.upper()\n",
    "    ref = ref.split(\" \")\n",
    "    print('ABAP Code ref:\\n', ref)\n",
    "\n",
    "    src = generated_code.upper()\n",
    "    src=src.split(\" \")\n",
    "    print('Generated Code\\n',src)\n",
    "\n",
    "    reference = [ref]\n",
    "    candidate = src\n",
    "\n",
    "##w/o smoothing function\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    print('BLEU score w/o smoothing-> {}'.format(score))\n",
    "\n",
    "##Smoothing function\n",
    "    score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25), smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method7)\n",
    "    print('BLEU score with smoothing-> {}'.format(score))\n",
    "\n",
    "    print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "    print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "    print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "    print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f065e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score(ref_code,generated_code):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rouge3', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(ref_code,generated_code)\n",
    "    print('Rouge score\\n', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91830347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_match(prediction, truth):\n",
    "    print('Exact match\\n', int(normalize_text(prediction) == normalize_text(truth))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9842ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, n_heads = 1, n_rows = 1, n_cols = 1):\n",
    "    \n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='viridis')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                           rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a3c69",
   "metadata": {},
   "source": [
    "#### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"Write class object ZCL_TEST\"\n",
    "actual_abap_code   = \"DATA: lo_ref of ref to ZCL_TEST. CREATE Object: lo_ref.\" \n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c506b",
   "metadata": {},
   "source": [
    "#### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"Check lines of table lt_zdest\"\n",
    "actual_abap_code   = \"if lines(lt_zdest) EQ 0. Endif.\" \n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b64434f",
   "metadata": {},
   "source": [
    "#### Example 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"Write function module z_fm1\"\n",
    "actual_abap_code   = \"call function 'Z_FM1'.\" \n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf1c15",
   "metadata": {},
   "source": [
    "#### Example 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"remove duplicate data from lt_data\"\n",
    "actual_abap_code   = \"delete adjacent duplicates from lt_data.\" \n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e3b9a",
   "metadata": {
    "id": "559f30b6"
   },
   "source": [
    "#### Example 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57147aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"Read information from the internal table lt_xyz\"\n",
    "actual_abap_code   = \"read table lt_xyz from data(ls_data). If sy-subrc = 0. endif.\" \n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbda57",
   "metadata": {},
   "source": [
    "#### Example 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165dd346",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"fetch date into a variable\"\n",
    "actual_abap_code   = \"data(lv_date) = sy-datum.\" \n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86923030",
   "metadata": {},
   "source": [
    "#### Example 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d49e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"remove leading zero from lv_data\"\n",
    "actual_abap_code   = \"SHIFT lv_data LEFT DELETING LEADING '0'.\" \n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8b985",
   "metadata": {},
   "source": [
    "#### Example 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"Get single from Database\"\n",
    "actual_abap_code   = \"SELECT SINGLE * FROM ztab INTO @DATA(ls_data).\"\n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b852a1",
   "metadata": {},
   "source": [
    "#### Example 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"Generate exception\"\n",
    "actual_abap_code   = \"raise exception e1.\"\n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad134b",
   "metadata": {},
   "source": [
    "#### Example 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb405e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"Do commit to database.\"\n",
    "actual_abap_code   = \"commit work.\"\n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76871d51",
   "metadata": {},
   "source": [
    "#### Example 11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b230f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pseudo_code = actual_abap_code = ''\n",
    "actual_pseudo_code = \"process over internal table lt_post to fetch data\"\n",
    "actual_abap_code   = \"loop at lt_post into data(ls_data). Endloop.\"\n",
    "df2,src, translation, attention = conv(actual_pseudo_code,actual_abap_code)\n",
    "ref = df2['Actual ABAP code ref'][0]\n",
    "generated = df2['Generated ABAP Code'][0]\n",
    "bleu_score(ref,generated)\n",
    "rouge_score(ref,generated)\n",
    "compute_exact_match(ref,generated)\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9c352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
